{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/qda.py:6: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import collections\n",
    "import datetime\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import PIL\n",
    "import pylab\n",
    "import scipy\n",
    "#import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "import statsmodels as sm\n",
    "import xgboost as xgb\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#sns.set(font_scale=1.0)\n",
    "mpl.rcParams['figure.figsize'] = 10, 6\n",
    "#sns.set_style('whitegrid')\n",
    "#sns.set_palette(sns.color_palette('muted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Raw Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Text from the Web and from Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Electronic Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n",
      "1176896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://www.gutenberg.org/files/2554/2554.txt\"\n",
    "raw = urlopen(url).read()\n",
    "print type(raw)\n",
    "print len(raw)\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "254352\n",
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(raw)\n",
    "print type(tokens)\n",
    "print len(tokens)\n",
    "print tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.text.Text'>\n",
      "['AND', 'PUNISHMENT', 'PART', 'I', 'CHAPTER', 'I', 'On', 'an', 'exceptionally', 'hot', 'evening', 'early', 'in', 'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in', 'which', 'he', 'lodged', 'in', 'S.', 'Place', 'and', 'walked', 'slowly', ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K.']\n",
      "\n",
      "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
      "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
      "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
      "great deal; Nikodim Fomitch; young man; Ilya Petrovitch; n't know;\n",
      "Project Gutenberg; Dmitri Prokofitch; Andrey Semyonovitch; Hay Market\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(tokens)\n",
    "print type(text)\n",
    "print text[1020:1060]\n",
    "print\n",
    "print text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5338"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.find(\"PART I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1157746"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.rfind(\"End of Project Gutenberg's Crime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\n"
     ]
    }
   ],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = urlopen(url).read()\n",
    "print html[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'BBC', u'NEWS', u'|', u'Health', u'|', u'Blondes', u\"'to\", u'die', u'out', u'in']\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "raw = soup.get_text()\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "print tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 5 matches:\n",
      "hey say too few people now carry the gene for blondes to last beyond the next \n",
      "blonde hair is caused by a recessive gene . In order for a child to have blond\n",
      " have blonde hair , it must have the gene on both sides of the family in the g\n",
      "ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
      "des would disappear is if having the gene was a disadvantage and I do not thin\n"
     ]
    }
   ],
   "source": [
    "tokens = tokens[96:399]\n",
    "text = nltk.Text(tokens)\n",
    "text.concordance('gene')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Local Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('data/the_old_man_and_the_sea.txt')\n",
    "raw = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('data/the_old_man_and_the_sea.txt')\n",
    "raw = f.read()\n",
    "type(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(raw)\n",
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [w.lower() for w in tokens]\n",
    "type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(words))\n",
    "type(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regular Expressions for Detecting Word Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Basic Metacharacters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'abaissed',\n",
       " u'abandoned',\n",
       " u'abased',\n",
       " u'abashed',\n",
       " u'abatised',\n",
       " u'abed',\n",
       " u'aborted',\n",
       " u'abridged',\n",
       " u'abscessed',\n",
       " u'absconded']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
    "[w for w in wordlist if re.search('ed$', w)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'abjectly',\n",
       " u'adjuster',\n",
       " u'dejected',\n",
       " u'dejectly',\n",
       " u'injector',\n",
       " u'majestic',\n",
       " u'objectee',\n",
       " u'objector',\n",
       " u'rejecter',\n",
       " u'rejector']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('^..j..t..$', w)][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranges and Closures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'gold', u'golf', u'hold', u'hole']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee',\n",
       " u'miiiiiinnnnnnnnnneeeeeeee',\n",
       " u'mine',\n",
       " u'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
    "[w for w in chat_words if re.search('^m+i+n+e+$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'a',\n",
       " u'aaaaaaaaaaaaaaaaa',\n",
       " u'aaahhhh',\n",
       " u'ah',\n",
       " u'ahah',\n",
       " u'ahahah',\n",
       " u'ahh',\n",
       " u'ahhahahaha',\n",
       " u'ahhh',\n",
       " u'ahhhh']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in chat_words if re.search('^[ha]+$', w)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.0085',\n",
       " u'0.05',\n",
       " u'0.1',\n",
       " u'0.16',\n",
       " u'0.2',\n",
       " u'0.25',\n",
       " u'0.28',\n",
       " u'0.3',\n",
       " u'0.4',\n",
       " u'0.5']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "[w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'C$', u'US$']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[A-Z]+\\$$', w)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1614',\n",
       " u'1637',\n",
       " u'1787',\n",
       " u'1901',\n",
       " u'1903',\n",
       " u'1917',\n",
       " u'1925',\n",
       " u'1929',\n",
       " u'1933',\n",
       " u'1934']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[0-9]{4}$', w)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'black-and-white',\n",
       " u'bread-and-butter',\n",
       " u'father-in-law',\n",
       " u'machine-gun-toting',\n",
       " u'savings-and-loan']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Applications of Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing More with Word Pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   e   i   o   u \n",
      "k 418 148  94 420 173 \n",
      "p  83  31 105  34  51 \n",
      "r 187  63  84  89  79 \n",
      "s   0   0 100   2   1 \n",
      "t  47   8   0 148  37 \n",
      "v  93  27 105  48  49 \n"
     ]
    }
   ],
   "source": [
    "rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')\n",
    "cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "cfd = nltk.ConditionalFreqDist(cvs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Examining the rows for s and t, we see they are in partial\n",
    "# “complementary distribution,” which is evidence that they are\n",
    "# not distinct phonemes in the language. Thus, we could conceivably\n",
    "# drop s from the Rotokas alphabet and simply have a pronunciation\n",
    "# rule that the letter t is pronounced s when followed by i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Word Stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ing']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
    "        if word.endswith(suffix): return word[:-len(suffix)]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'potato'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem('potatoes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('process', 'ing')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('process', 'es')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem2(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DENNIS : Listen , strange women ly in pond distribut sword i no basi for a system of govern . Supreme execut power deriv from a mandate from the mass , not from some farcical aquatic ceremony .'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government. Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "' '.join([stem2(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching Tokenized Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "brave; brave; brave\n"
     ]
    }
   ],
   "source": [
    "moby = nltk.Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))\n",
    "moby.findall(r\"<a> (<.*>) <man>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you rule bro; telling you bro; u twizted bro\n"
     ]
    }
   ],
   "source": [
    "chat = nltk.Text(nltk.corpus.nps_chat.words())\n",
    "chat.findall(r\"<.*> <.*> <bro>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\n",
      "la la; lovely lol lol love; lol lol lol.; la la la; la la la\n"
     ]
    }
   ],
   "source": [
    "chat.findall(r\"<l.*>{3,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed and other activities; water and other liquids; tomb and other\n",
      "landmarks; Statues and other monuments; pearls and other jewels;\n",
      "charts and other items; roads and other features; figures and other\n",
      "objects; military and other areas; demands and other factors;\n",
      "abstracts and other compilations; iron and other metals\n"
     ]
    }
   ],
   "source": [
    "hobbies_learned = nltk.Text(nltk.corpus.brown.words(categories=['hobbies', 'learned']))\n",
    "hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DENNIS : Listen , strange women lying in ponds distributing swords is no basis for a system of government . Supreme executive power derives from a mandate from the masses , not from some farcical aquatic ceremony .'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government. Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'denni : listen , strang women lie in pond distribut sword is no basi for a system of govern . suprem execut power deriv from a mandat from the mass , not from some farcic aquat ceremoni .'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "' '.join([porter.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'den : list , strange wom lying in pond distribut sword is no bas for a system of govern . suprem execut pow der from a mand from the mass , not from som farc aqu ceremony .'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "' '.join([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'DENNIS : Listen , strange woman lying in pond distributing sword is no basis for a system of government . Supreme executive power derives from a mandate from the mass , not from some farcical aquatic ceremony .'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "' '.join([wnl.lemmatize(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions for Tokenizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Approaches to Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK's Regular Expression Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'U.S.A.', 'poster', '-', 'print', 'costs', '$', '12.40...']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "pattern = r'''[\\d.]+|[A-Z][.A-Z]+\\b\\.*|\\w+|\\S'''\n",
    "nltk.regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Issues with Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.250994070456922"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0 * len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the wild events which were to follow this girl had no\n",
      "part at all; he never saw her again until all his tale was over.\n",
      "\n",
      "And yet, in some indescribable way, she kept recurring like a\n",
      "motive in music through all his mad adventures afterwards, and the\n",
      "glory of her strange hair ran like a red thread through those dark\n",
      "and ill-drawn tapestries of the night.\n",
      "\n",
      "For what followed was so\n",
      "improbable, that it might well have been a dream.\n",
      "\n",
      "When Syme went out into the starlit street, he found it for the\n",
      "moment empty.\n",
      "\n",
      "Then he realised (in some odd way) that the silence\n",
      "was rather a living silence than a dead one.\n",
      "\n",
      "Directly outside the\n",
      "door stood a street lamp, whose gleam gilded the leaves of the tree\n",
      "that bent out over the fence behind him.\n",
      "\n",
      "About a foot from the\n",
      "lamp-post stood a figure almost as rigid and motionless as the\n",
      "lamp-post itself.\n",
      "\n",
      "The tall hat and long frock coat were black; the\n",
      "face, in an abrupt shadow, was almost as dark.\n",
      "\n",
      "Only a fringe of\n",
      "fiery hair against the light, and also something aggressive in the\n",
      "attitude, proclaimed that it was the poet Gregory.\n",
      "\n",
      "He had something\n",
      "of the look of a masked bravo waiting sword in hand for his foe.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
    "sents = sent_tokenizer.tokenize(text)\n",
    "for s in sents[171:181]:\n",
    "    print s\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segment(text, segs):\n",
    "    words = []\n",
    "    last = 0\n",
    "    for i in range(len(segs)):\n",
    "        if segs[i] == '1':\n",
    "            words.append(text[last:i+1])\n",
    "            last = i+1\n",
    "    words.append(text[last:])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment(text, seg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do', 'you', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'do', 'you']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment(text, seg2)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(text, segs):\n",
    "    words = segment(text, segs)\n",
    "    text_size = len(words)\n",
    "    lexicon_size = len(' '.join(list(set(words))))\n",
    "    return text_size + lexicon_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
    "seg3 = \"0000100100000011001000000110000100010000001100010000001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doyou',\n",
       " 'see',\n",
       " 'thekitt',\n",
       " 'y',\n",
       " 'see',\n",
       " 'thedogg',\n",
       " 'y',\n",
       " 'doyou',\n",
       " 'like',\n",
       " 'thekitt']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment(text, seg3)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(text, seg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(text, seg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(text, seg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 ['doyouseethe', 'k', 'itt', 'yseet', 'hedoggy', 'doyoul', 'iket', 'hekittyl', 'iket', 'hedoggy']\n",
      "61 ['doyouseethe', 'ki', 'ttyseet', 'hedoggy', 'doyouliket', 'hekittyliket', 'hedoggy']\n",
      "59 ['doyouseeth', 'eki', 'ttyseet', 'hedoggy', 'doyoulikethekittyliket', 'hedoggy']\n",
      "57 ['doyouseethekitty', 'seet', 'hedoggy', 'doyoulikethekittyliket', 'hedoggy']\n",
      "55 ['doyou', 'see', 'thekitty', 'see', 't', 'hedoggy', 'doyou', 'likethekittyliket', 'hedoggy']\n",
      "54 ['doyou', 'see', 'thek', 'itty', 'see', 'thedoggy', 'doyou', 'likethekittylike', 'thedoggy']\n",
      "53 ['doyou', 'see', 'thek', 'itty', 'see', 'thedoggy', 'doyou', 'likethek', 'itty', 'like', 'thedoggy']\n",
      "52 ['doyou', 'see', 'thek', 'ittysee', 'thedoggy', 'doyou', 'like', 'thek', 'itty', 'like', 'thedoggy']\n",
      "45 ['doyou', 'see', 'thek', 'itty', 'see', 'thedoggy', 'doyou', 'like', 'thek', 'itty', 'like', 'thedoggy']\n",
      "42 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "\n",
      "42 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0000100100000001001000000010000100010000000100010000000'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Non-deterministic search using simulated annealing: Begin searching with\n",
    "# phrase segmentations only; randomly perturb the zeros and ones proportional\n",
    "# to the “temperature”; with each iteration the temperature is lowered and\n",
    "# the perturbation of boundaries is reduced.\n",
    "\n",
    "def flip(segs, pos):\n",
    "    return segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:]\n",
    "\n",
    "def flip_n(segs, n):\n",
    "    for i in range(n):\n",
    "        segs = flip(segs, random.randint(0,len(segs)-1))\n",
    "    return segs\n",
    "\n",
    "def anneal(text, segs, iterations, cooling_rate):\n",
    "    temperature = float(len(segs))\n",
    "    while temperature > 0.5:\n",
    "        best_segs, best = segs, evaluate(text, segs)\n",
    "        for i in range(iterations):\n",
    "            guess = flip_n(segs, int(round(temperature)))\n",
    "            score = evaluate(text, guess)\n",
    "            if score < best:\n",
    "                best, best_segs = score, guess\n",
    "                score, segs = best, best_segs\n",
    "                print evaluate(text, segs), segment(text, segs)\n",
    "        temperature = temperature / cooling_rate\n",
    "    print\n",
    "    print evaluate(text, segs), segment(text, segs)\n",
    "    return segs\n",
    "\n",
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "anneal(text, seg1, 5000, 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '.', '.', '.', '.', '.', \"'\", '.', '.', '-']\n",
      "['1899', '1961', '1952', '1952', '2012', '2012', '1952', '1952']\n"
     ]
    }
   ],
   "source": [
    "# Save some text into a file corpus.txt. Define a function load(f) that reads from the\n",
    "# file named in its sole argument, and returns a string containing the text of the file.\n",
    "\n",
    "# Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of\n",
    "# punctuation in this text. Use one multiline regular expression inline comments, using the\n",
    "# verbose flag (?x).\n",
    "\n",
    "def load(f):\n",
    "    text = None\n",
    "    with open(f) as file:\n",
    "        text = file.read()\n",
    "    pattern = r'''(?x)\n",
    "\\.\\.\\.\n",
    "|\\.\n",
    "|\\?\n",
    "|\\!\n",
    "|-\n",
    "|\\'\n",
    "|\\\"'''\n",
    "    punc = nltk.regexp_tokenize(text, pattern)\n",
    "    return punc\n",
    "\n",
    "print load('data/the_old_man_and_the_sea.txt')[:10]\n",
    "\n",
    "# Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following kinds of expressions:\n",
    "# monetary amounts; dates; names of people and organizations.\n",
    "\n",
    "def load2(f):\n",
    "    text = None\n",
    "    with open(f) as file:\n",
    "        text = file.read()\n",
    "    pattern = r'''(?x)\n",
    "\\d{4}\n",
    "'''\n",
    "    tokens = nltk.regexp_tokenize(text, pattern)\n",
    "    return tokens\n",
    "\n",
    "print load2('data/the_old_man_and_the_sea.txt')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bar', 'baz', 'foo']\n",
      "['foo', 'bar', 'baz']\n",
      "['bar', 'baz', 'foo']\n"
     ]
    }
   ],
   "source": [
    "# Create a variable words containing a list of words. Experiment\n",
    "# with words.sort() and sorted(words). What is the difference?\n",
    "\n",
    "words = ['foo', 'bar', 'baz']\n",
    "print sorted(words)\n",
    "print words\n",
    "words.sort()\n",
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['who',\n",
       " 'which',\n",
       " 'which',\n",
       " 'where',\n",
       " 'Why',\n",
       " 'what',\n",
       " 'where',\n",
       " 'who',\n",
       " 'where',\n",
       " 'When']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in some text from a corpus, tokenize it, and print the list of all wh-word\n",
    "# types that occur. (wh-words in English are used in questions, relative clauses,\n",
    "# and exclamations: who, which, what, and so on.) Print them in order. Are any words\n",
    "# duplicated in this list, because of the presence of case distinctions or punctuation?\n",
    "\n",
    "text = None\n",
    "with open('data/the_old_man_and_the_sea.txt') as file:\n",
    "        text = file.read()\n",
    "words = nltk.word_tokenize(text)\n",
    "wh_words = [w for w in words if w.lower().startswith('wh')]\n",
    "wh_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a    e    i    o    u    á    é    í    ó    ö    ú    ü    ű \n",
      "a 4419 2622 1983 2510  574 1654  793  220  696  503  188  142   24 \n",
      "e 2716 7350 1769 1256  316  761 2118  230  198  400  118  581  165 \n",
      "i 1500 1777  414  715  229  813  475   68  161  161   69   59   16 \n",
      "o 2682 1072  577 1329  257 1094  315   88  274  110  100   56   10 \n",
      "u  634  188  131  245   43  394   45   22  104   15    2    1    0 \n",
      "á 2123  708  529  968  192  614  331   95  325  102   81   33    9 \n",
      "é  884 2345  507  290   77  277  590   44   60  110   38  169   22 \n",
      "í  135  157   41  142   19  156   83    4   50    5    3   15   29 \n",
      "ó  631  335  178  222   66  191  143   44   46   53   24   15    4 \n",
      "ö  171  768  159   73   20   39  321    9   11  508    9   68   21 \n",
      "ú  200   79   68  119   12   71   34   12   20   24    7    5    1 \n",
      "ü  195  452   78   75   14   34  128    2    5  158    8   19    2 \n",
      "ű   39  124   23   20    5   12   37    1    2   28    5    7   17 \n"
     ]
    }
   ],
   "source": [
    "# Download some text from a language that has vowel harmony (e.g., Hungarian),\n",
    "# extract the vowel sequences of words, and create a vowel bigram table.\n",
    "\n",
    "text = None\n",
    "with open('data/az_elveszett_cirkalo_utf8.txt') as file:\n",
    "    text = file.read()\n",
    "p = r'a|á|e|é|i|í|o|ó|ö|ő|u|ú|ü|ű'.decode('utf-8')\n",
    "vowels = nltk.regexp_tokenize(text.lower().decode('utf-8'), p)\n",
    "fd = nltk.ConditionalFreqDist(nltk.bigrams(vowels))\n",
    "fd.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure      4.084\n",
      "belles_lettres 10.988\n",
      "editorial      9.471\n",
      "fiction        4.910\n",
      "government     12.084\n",
      "hobbies        8.922\n",
      "humor          7.888\n",
      "learned        11.926\n",
      "lore           10.255\n",
      "mystery        3.834\n",
      "news           10.177\n",
      "religion       10.203\n",
      "reviews        10.770\n",
      "romance        4.349\n",
      "science_fiction4.978\n"
     ]
    }
   ],
   "source": [
    "# Readability measures are used to score the reading difficulty of a text, for the\n",
    "# purposes of selecting texts of appropriate difficulty for language learners.\n",
    "# Let us define mu_w to be the average number of letters per word, and mu_s to be the average\n",
    "# number of words per sentence, in a given text. The Automated Readability Index (ARI) of\n",
    "# the text is defined to be: 4.71 mu_w + 0.5 mu_s - 21.43. Compute the ARI score for various\n",
    "# sections of the Brown Corpus, including section f (popular lore) and j (learned). Make\n",
    "# use of the fact that nltk.corpus.brown.words() produces a se- quence of words, whereas\n",
    "# nltk.corpus.brown.sents() produces a sequence of sentences.\n",
    "\n",
    "def num_chars_per_word(ws):\n",
    "    n, s = 0, 0\n",
    "    for w in ws:\n",
    "        n += 1\n",
    "        s += len(w)\n",
    "    return 1.0 * s / n\n",
    "\n",
    "def num_words_per_sents(ss):\n",
    "    n, s = 0, 0\n",
    "    for se in ss:\n",
    "        n += 1\n",
    "        s += len(se)\n",
    "    return 1.0 * s / n\n",
    "\n",
    "for c in nltk.corpus.brown.categories():\n",
    "    mu_w = num_chars_per_word(nltk.corpus.brown.words(categories=c))\n",
    "    mu_s = num_words_per_sents(nltk.corpus.brown.sents(categories=c))\n",
    "    print '{0:15}{1:2.3f}'.format(c, 4.71*mu_w + 0.5*mu_s - 21.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original            porter              lancaster           \n",
      "------------------------------------------------------------\n",
      "Morgan              morgan              morg                \n",
      "Ann                 ann                 an                  \n",
      "Turner              turner              turn                \n",
      "was                 wa                  was                 \n",
      "well                well                wel                 \n",
      "certainly           certainli           certain             \n",
      "wife                wife                wif                 \n",
      "was                 wa                  was                 \n",
      "Ann                 ann                 an                  \n",
      "married             marri               marry               \n",
      "have                have                hav                 \n",
      "all                 all                 al                  \n",
      "was                 wa                  was                 \n",
      "rationalization     ration              rat                 \n",
      "woke                woke                wok                 \n",
      "Ann                 ann                 an                  \n",
      "His                 hi                  his                 \n"
     ]
    }
   ],
   "source": [
    "ws = nltk.corpus.brown.words(categories='adventure')[:100]\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "ws_por = [porter.stem(w).lower() for w in ws]\n",
    "\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "ws_lan = [lancaster.stem(w).lower() for w in ws]\n",
    "\n",
    "print '{0:20}{1:20}{2:20}'.format('original', 'porter', 'lancaster')\n",
    "print '-' * 60\n",
    "for i in range(len(ws)):\n",
    "    if ws_por[i] != ws_lan[i]:\n",
    "        print '{0:20}{1:20}{2:20}'.format(ws[i], ws_por[i], ws_lan[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will wind up its 1961 session Monday and head for home -- where some of the highw\n",
      "aymond H. Hawksley , the present city CD head , believes . Mr. Hawksley said yest\n",
      "ance as Mayor . In an apparent effort to head off such a rival primary slate , Mr\n",
      "political affairs officer , a department head or an economist , to start . Each m\n",
      "munist bloc , and Prince Souphanouvong , head of the pro-Communist Pathet Lao for\n",
      "on to the union . Division three will be headed by the Marines followed by 12 sta\n",
      "d by 12 states ; ; division four will be headed by the Navy , followed by 11 stat\n",
      "owed by 11 states . Division six will be headed by the Coast Guard , followed by \n",
      " Future clouded Barnett , as the titular head of the Democratic party , apparentl\n",
      "n. Gursel as president . Gen. Gursel has headed the military junta the last 17 mo\n",
      "hrew it over first baseman Throneberry's head Brandt took third and Gentile secon\n",
      "as the Bird third sacker grabbed a bat , headed for the plate and bounced a third\n",
      " and was able to save place money only a head in front of Glen T. Hallowell's Mil\n",
      "olve that quarterback problem just as we head that way '' . Ramsey has a thing or\n",
      ". She said , when she learned Jackie was heading home : `` I'm just speculating ,\n",
      " . You'll probably get a ball bat on the head . He's mad at the world '' . But   \n",
      "ladelphia Eagles , was elevated today to head coach . Skorich received a three-ye\n",
      "Norm Van Brocklin permission to seek the head coaching job with the Minnesota Vik\n",
      "ning the championship . Shaw and Skorich headed a group of players , coaches and \n",
      "Named by Mayor Wagner three years ago to head a committee that included James A. \n",
      "adium in works The New York franchise is headed by Mrs. Charles Shipman Payson . \n",
      "     figure , that 12 '' , he said as he headed for the clubhouse , not too much \n",
      " in the Teter House . Mrs. Roger Mead is head of the luncheon table decorations .\n",
      "s and comic books in the back seat , and head for home . And where is `` home '' \n",
      "Clifton Lisle , of Chester Springs , who headed the Troop Committee for much of i\n",
      "at Kercheval Monday afternoon when a car heading north on Belvidere stopped belat\n",
      "nd working conditions . Thomas Rotelli , head of Rhode Island Incinerator Service\n",
      "r , also one of the youngest men ever to head a major American university , succe\n",
      " . Those that remain are those that were headed by strong executives , men with t\n",
      "970s here Tuesday by Peter G. Peterson , head of one of the world's greatest came\n",
      "e public through a group of underwriters headed by Dillon , Read & Co. , to raise\n",
      "ommittee , and Mr. Chandler . Mr. Bolker heads a group within the building and de\n",
      " three to four inches above the puppet's head . Make sure that the metal tube thr\n",
      "ciate curator of education . Miss Bouton headed up one of the four groups that we\n",
      "ost prominent leader , Albert Pike , who headed the Scottish Rite from 1859 to 18\n",
      "ulogized . C. Wheeler Barnes of Denver , head of the Scottish Rite in Colorado , \n",
      "that these could not be talked away at a heads of government meeting . He wanted \n",
      "ar . Road to Vienna The U. S. and Soviet heads of Government have met three times\n",
      "eadle has stuck close to his research as head of the school's famous biology divi\n",
      "paratroops to the Congo . On July 11 the head of the mineral-rich province of Kat\n",
      "vador saw the youth hold his against the head of stewardess Lois Carnegey ; ; the\n",
      "Lois Carnegey ; ; the man put his at the head of Capt. Byron D. Rickards . To Ric\n",
      "est hurdle the Republicans would have to face is a state law which says that befo\n",
      "vorable report , although the resolution faces hard sledding later . The house pa\n",
      "St. Louis sitting in Cook County court . Faced seven cases Karns had been schedul\n",
      "e difficulties , changes , and tragedies facing them , and other allies who intel\n",
      " ways '' , Jones said . The state is now faced with the immediate question of rai\n",
      "immie H. Davis administration appears to face a difficult year in 1961 , with the\n",
      "y other tax bill , it could very well be faced this spring at the fiscal session \n",
      "sly adopted , were felt necessary in the face of modern trends away from the Bibl\n",
      " contest and only five batters needed to face him before there existed a 3-to-3 d\n",
      "mpe worked a walk as the first batter to face Hyde and romped around as Siebern b\n",
      "ams play in baseball parks and therefore face an early-season conflict in dates .\n",
      " job . A thug struck a cab driver in the face with a pistol last night after robb\n",
      " boy received second-degree burns of the face , neck and back . His condition was\n",
      "ourteen years is the maximum penalty now faced by the new five , who may have alt\n",
      "k his uncle with a rock , scratching his face . He also struck his aunt and wife \n",
      "week and will be returned to Portland to face charges of assault and robbery , Po\n",
      "ent age and suddenly realizing that they face an estate tax problem with their cl\n",
      "ent neo-stagnationist thesis that we are facing a future of limited and slow grow\n",
      " `` As I see it , this country has never faced such great dangers as threaten us \n",
      "ro , `` for unless we do we're liable to face similar situations in this hemisphe\n",
      "d E. Perlman said Tuesday his line would face the threat of bankruptcy if the Che\n",
      ". Most children love the animated puppet faces and their flexible bodies , and th\n",
      "er sampled the exotic , peppery fare . ) faces in places Pualani and Randy Avon ,\n",
      " all of it is totally meaningless in the face of the Kirov's utterly captivating \n",
      "k while he throws pillow feathers in her face ; ; a frigid beauty , and three sil\n",
      "owever , to enjoy the music or watch the faces of the delighted audience . She is\n",
      " approached the open bandstand , erected facing the South entrance to the Executi\n",
      "g the coveted promotion , not only is he faced with the problem of starting over \n",
      "a legal profession . The problem must be faced squarely . If laborers are merely \n",
      "s . The liberals , smelling blood , were faced with the necessity of winning thre\n",
      "Inauguration . One day last week , Nixon faced a painful constitutional chore tha\n",
      "nd by his kind , strong and somewhat sly face '' . G. David Thompson is one of th\n",
      "ht note : The gains were achieved in the face of temporary traffic lags late in 1\n",
      "t in revenues , holders of the bonds are faced with more of the same . These , ho\n",
      "It's a tough combination for the U.S. to face . `` Olivetti had a special interes\n",
      "resent budget problem all libraries must face , from the largest to the smallest \n"
     ]
    }
   ],
   "source": [
    "# Use WordNet to create a semantic index for a text collection. Extend the concordance\n",
    "# search program in Example 3-1, indexing each word using the offset of its first synset,\n",
    "# e.g., wn.synsets('dog')[0].offset (and optionally the offset of some of its ancestors\n",
    "# in the hypernym hierarchy).\n",
    "\n",
    "class IndexedText(object):\n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        wn = nltk.corpus.wordnet\n",
    "        syns = [(wn.synsets(self._stem(word).lower())[0].name(),\n",
    "             i)\n",
    "            for (i, word) in enumerate(text) if len(wn.synsets(self._stem(word).lower())) > 0]\n",
    "        parts = [(wn.synsets(self._stem(word).lower())[0].part_holonyms()[0].name(),\n",
    "             i)\n",
    "            for (i, word) in enumerate(text) if (\n",
    "                len(wn.synsets(self._stem(word).lower())) > 0\n",
    "                and len(wn.synsets(self._stem(word).lower())[0].part_holonyms()) > 0)]\n",
    "        self._index = nltk.Index(syns + parts)\n",
    "    \n",
    "    def concordance(self, word, width=40):\n",
    "        key = self._stem(word)\n",
    "        wc = width/4 # words of context\n",
    "        for i in self._index[key]:\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            ldisplay = '%*s' % (width, lcontext[-width:])\n",
    "            rdisplay = '%-*s' % (width, rcontext[:width])\n",
    "            print ldisplay, rdisplay\n",
    "            \n",
    "    def _stem(self, word):\n",
    "        return self._stemmer.stem(word).lower()\n",
    "    \n",
    "text = nltk.Text(nltk.corpus.brown.words(categories='news'))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "it = IndexedText(stemmer, text)\n",
    "it.concordance('head.n.01')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
